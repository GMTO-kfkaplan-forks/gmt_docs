

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>6. Platform &mdash; GMT Software And Controls documentation</title>
  

  
  
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../../_static/css/additional_style.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="../../genindex.html"/>
        <link rel="search" title="Search" href="../../search.html"/>
    <link rel="top" title="GMT Software And Controls documentation" href="../../index.html"/>
        <link rel="up" title="Architecture and Design" href="../index.html"/>
        <link rel="next" title="7. Process" href="../process/process_introduction.html"/>
        <link rel="prev" title="5. Frameworks" href="../frameworks/component_frameworks.html"/> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../index.html" class="icon icon-home"> GMT Software And Controls
          

          
            
            <img src="../../_static/logowhite.png" class="logo" />
          
          </a>

          
            
            
              <div class="version">
                1.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../release_notes/index.html">Release Notes</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Architecture and Design</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../introduction/swcs_introduction.html">1. Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../architecture/overall_architecture.html">2. Overall Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tcs/tcs_introduction.html">3. Telescope Control System</a></li>
<li class="toctree-l2"><a class="reference internal" href="../services/observatory_services.html">4. Observatory Services</a></li>
<li class="toctree-l2"><a class="reference internal" href="../frameworks/component_frameworks.html">5. Frameworks</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">6. Platform</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#computing-platform">6.1. Computing Platform</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#computer-hardware">6.1.1. <em>Computer Hardware</em></a></li>
<li class="toctree-l4"><a class="reference internal" href="#plcs">6.1.2. <em>PLCs</em></a></li>
<li class="toctree-l4"><a class="reference internal" href="#operating-systems">6.1.3. <em>Operating Systems</em></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#field-device-interface-technology">6.2. Field Device Interface Technology</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#ethercat">6.2.1. <em>EtherCAT</em></a></li>
<li class="toctree-l4"><a class="reference internal" href="#electronic-cabinets">6.2.2. <em>Electronic Cabinets</em></a></li>
<li class="toctree-l4"><a class="reference internal" href="#terminal-blocks">6.2.3. <em>Terminal Blocks</em></a></li>
<li class="toctree-l4"><a class="reference internal" href="#integrated-prototypes">6.2.4. <em>Integrated Prototypes</em></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#time-distribution-system">6.3. Time Distribution System</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#time-distribution-protocols">6.3.1. <em>Time Distribution Protocols</em></a></li>
<li class="toctree-l4"><a class="reference internal" href="#precision-time-protocol">6.3.2. <em>Precision Time Protocol</em></a></li>
<li class="toctree-l4"><a class="reference internal" href="#ptp-software-stacks-for-linux">6.3.3. <em>PTP Software Stacks for Linux</em></a></li>
<li class="toctree-l4"><a class="reference internal" href="#lab-evaluation">6.3.4. <em>Lab Evaluation</em></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#distributed-communications-middleware">6.4. Distributed Communications - Middleware</a></li>
<li class="toctree-l3"><a class="reference internal" href="#networking">6.5. Networking</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#control-network">6.5.1. <em>Control Network</em></a></li>
<li class="toctree-l4"><a class="reference internal" href="#ultra-low-latency-network">6.5.2. <em>Ultra-Low Latency Network</em></a></li>
<li class="toctree-l4"><a class="reference internal" href="#benchmark-tests">6.5.3. <em>Benchmark Tests</em></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#software-development-platform">6.6. Software Development Platform</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../process/process_introduction.html">7. Process</a></li>
<li class="toctree-l2"><a class="reference internal" href="../risks/risks.html">8. Software and Control Risks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../bibliography/bibliography.html">9. Bibliography</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../SWCS_Standards/index.html">Software and Controls Standards</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../software_development/index.html">Software Development</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../../index.html">GMT Software And Controls</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          

 



<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../index.html">Docs</a> &raquo;</li>
      
          <li><a href="../index.html">Architecture and Design</a> &raquo;</li>
      
    <li>6. Platform</li>
      <li class="wy-breadcrumbs-aside">
        
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="platform">
<span id="id1"></span><h1>6. Platform<a class="headerlink" href="#platform" title="Permalink to this headline">¶</a></h1>
<p>This section provides a description of the technologies used in the development
of the GMT SWC System.  It is the responsibility of the SWCS to define the
platform that can meet operational requirements, thus most of the
platform-related requirements are located in Hardware and Software Standard
documents.</p>
<p>Several criteria play an important role in evaluating the platform technologies:
technical, long-term availability, commercial and community support, and cost
effectiveness. Fortunately, several choices are available both in the commercial
and open source world to address the requirements of the GMT Software and
Controls, making feasible several architecture platforms that will provide an
adequate implementation. The SWCS platform is based on the use of
well-established industrial standards, commercial off-the-shelf, and open source
products while custom-built solutions are strongly discouraged. Design choices
and prescribed standards come about through prototyping, benchmarking and
evaluations. Other guidelines considered include:</p>
<blockquote>
<div><ul class="simple">
<li>Avoiding proprietary or vendor lock-in options when several solutions based
on open standards or even open source already exist,</li>
<li>Avoiding niche market solutions when alternatives exist from competitive
markets, especially where economy of scale plays an important role (e.g.,
industrial automation).</li>
</ul>
</div></blockquote>
<p>The GMT project office has evaluated <a class="reference internal" href="../bibliography/bibliography.html#filg12" id="id2">[Filg12]</a> several technologies as part of
its preliminary design work, ranging from mature to more emergent. Preliminary
evaluations show that a design based on widely available industrial open
standards and open source products is capable of meeting the GMT Control System
requirements. A set of guidelines and requirements regarding the use of these
technologies in the project is included in the Software and Controls Handbook
<a class="reference internal" href="../bibliography/bibliography.html#filg13b" id="id3">[Filg13b]</a>.</p>
<div class="section" id="computing-platform">
<span id="id4"></span><h2>6.1. Computing Platform<a class="headerlink" href="#computing-platform" title="Permalink to this headline">¶</a></h2>
<p>Device Control Subsystems are deployed in standardized GMT Device Control
Computers (DCCs), while general processing components are deployed in Data
Processing Computers (DPCs).</p>
<p>Two types of DCCs are supported: Intel Industrial Computers (PCs) running Linux
and Programmable Logic Controllers (PLCs). In addition to the obvious benefits
of maintainability and cost-effectiveness, the use of standardized computer
architecture ensures that software components can adequately and robustly
compile and deploy on them.</p>
<p>PC based DCCs running Linux with real time extensions are used in the following
use cases:</p>
<blockquote>
<div><ul class="simple">
<li>The Device Control Subsystem has to be integrated with the low latency
network.</li>
<li>High computing performance is required.</li>
<li>Complex operation scenarios.</li>
</ul>
</div></blockquote>
<p>PLCs are used in the following use cases:</p>
<blockquote>
<div><ul class="simple">
<li>The bandwidth necessary for telemetry is low.</li>
<li>Simple supervision, coordination or operation scenarios.</li>
</ul>
</div></blockquote>
<div class="section" id="computer-hardware">
<h3>6.1.1. <em>Computer Hardware</em><a class="headerlink" href="#computer-hardware" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><p>The current baseline consists of standard off-the-shelf data center class
computers. The Table below presents the characteristics of the computer platform.
The interface with field devices is done using EtherCAT, a real-time Ethernet
based fieldbus, which only requires the availability of standard network
ports.  This is in contrast with backplane-based systems (e.g., VME, PXI)
where the field interface is implemented with cards connected to the
backplane. The Section on <a class="reference internal" href="#field-device-interface"><span class="std std-ref">Field Device Interface Technology</span></a> describes with more
detail the characteristics of EtherCAT. The fact that modern motor drivers
include embedded motion controllers and even some local logic by means of a
PLC reduces the performance requirements on the computing platform. As a
consequence computers can be very compact. Data center class computers are
used to improve system reliability (e.g., redundant power supplies). This
configuration has been used during prototyping activities. The physical and
power consumption envelope of these computers will be optimized once the
project approaches its deployment phase.</p>
<p>Some of the computer components are optional as their use depends on the
functions performed by the software components deployed in them. For example,
DCCs require access to the fieldbus, but many of them do not require access to
the low latency network. DPCs do not require access to the fieldbus, but require
high capacity storage for quick processing of intermediate results.</p>
<p>The table below shows the GMT Base Computer Specification:</p>
<table border="1" class="docutils" id="id16">
<caption><span class="caption-number">Table 24 </span><span class="caption-text">GMT Base Computer Specification</span><a class="headerlink" href="#id16" title="Permalink to this table">¶</a></caption>
<colgroup>
<col width="26%" />
<col width="30%" />
<col width="45%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head"><div class="first last line-block">
<div class="line">Category</div>
</div>
</th>
<th class="head"><div class="first last line-block">
<div class="line">Description</div>
</div>
</th>
<th class="head"><div class="first last line-block">
<div class="line">Function</div>
</div>
</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><div class="first last line-block">
<div class="line">Processor architecture</div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="first last line-block">
<div class="line">Mulcore Intel CPU</div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="first last line-block">
<div class="line">Data processing and control processes</div>
<div class="line">execution.  Hardware acceleration</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="first last line-block">
<div class="line">Storage</div>
</div>
</td>
<td><div class="first last line-block">
<div class="line">SATA/PCIe solid state disk</div>
</div>
</td>
<td><div class="first last line-block">
<div class="line">OS image, local telemetry circular buffer</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="first last line-block">
<div class="line">Memory</div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="first last line-block">
<div class="line">&gt; 4 GB RAM</div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="first last line-block">
<div class="line">Process memory, ultra-fast telemetry</div>
<div class="line">circular buffer</div>
</div>
</td>
</tr>
<tr class="row-odd"><td rowspan="3"><div class="first last line-block">
<div class="line">Networking</div>
<div class="line"><br /></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="first last line-block">
<div class="line">2 x 1 GbE</div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="first last line-block">
<div class="line">EtherCAT fieldbus network interface with</div>
<div class="line">cable redundancy.</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="first last line-block">
<div class="line">2 x 10 GbE</div>
</div>
</td>
<td><div class="first last line-block">
<div class="line">Observatory Service network interface</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="first last line-block">
<div class="line">1 x 40 Gbps Infiniband</div>
</div>
</td>
<td><div class="first last line-block">
<div class="line">Low latency network interface</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="first last line-block">
<div class="line">Hardware acceleration</div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="first last line-block">
<div class="line">PCIe GPU</div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="first last line-block">
<div class="line">Hardware acceleration of wavefront</div>
<div class="line">processing and reconstruction</div>
</div>
</td>
</tr>
</tbody>
</table>
<p>The figure below provides the description of the two control subsystem
deployment architectures. The choice between one over the other is part of the
TCS architecture design process and will be analyzed in a case-by-case basis
during detailed design. Examples of the Single Tier Control configuration are
the Mount Control System or the M1 Control System. The Enclosure Control System
follows the Two Tier Control design as it fits the capabilities of the
industrial companies that may procure the low-level control system integrated
with the rest of the enclosure.</p>
<div class="figure" id="id17">
<img alt="../../_images/controller-architecture.png" src="../../_images/controller-architecture.png" />
<p class="caption"><span class="caption-number">Fig. 36 </span><span class="caption-text">Single and Two tier Controller Architecture</span></p>
</div>
</div></blockquote>
</div>
<div class="section" id="plcs">
<h3>6.1.2. <em>PLCs</em><a class="headerlink" href="#plcs" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><p>Programmable Logic Controllers (PLCs, PACs and Soft PLCs) are used in several
specific areas, as in the case of the Enclosure for rotating the dome and
commanding shutters and wind vents. PLCs are required to use the IEC 61131-3
<a class="reference internal" href="../bibliography/bibliography.html#iec61131-3" id="id5">[IEC61131-3]</a> programming standard widely adopted in industry. This part of
IEC 61131 specifies the syntax and semantics of a unified suite of programming
languages for programmable controllers. These consist of two textual
languages: Instruction List and Structured Text, and two graphical languages:
Ladder Diagram and Function Block Diagram. Also, Sequential Function Chart
elements are defined for structuring the internal organization of programmable
controller programs and function blocks <a class="reference internal" href="../bibliography/bibliography.html#iec61131-3" id="id6">[IEC61131-3]</a>.</p>
<p>Programmable controllers are integrated with the rest of the Observatory using
the OPC Unified Architecture (OPC UA) communications standard <a class="reference internal" href="../bibliography/bibliography.html#soto13" id="id7">[Soto13]</a>.  <a class="reference external" href="http://www.opcfoundation.org/about/what-is-opc">OPC
UA</a>
is the next generation OPC standard that provides a cohesive, secure and
reliable cross platform framework for access to real time and historical data
and events.</p>
<p>PLCopen provides another important programming solution to the wide range of
selectable motion control hardware available. PLCopen motion standard provides
a way to have standard application libraries that are reusable for multiple
hardware platforms. This lowers development, maintenance, and support costs
while eliminating confusion. Standardization comes about by defining libraries
of reusable components so that the programming is less hardware dependent,
increasing the reusability of the application software, and enabling the
application to scale across different control solutions (see <a class="reference external" href="http://www.plcopen.org/">plcopen.org</a>).</p>
<p>Currently the suite of PLCopen Motion Control Specifications consists of 6
parts, of which GMT will adopt Part 1: <a class="reference external" href="http://www.plcopen.org/pages/tc2_motion_control/">Function Blocks for Motion Control</a>.</p>
<p>A Soft PLC is a control software engine that runs on standard CPUs such as
Intel, ARM, PowerPC, etc. Combining the features of a PLC (such as I/O control,
PID, ladder logic, standard industrial communication protocols) with the power
of computers (e.g., data handling, logging, running multiple applications
simultaneously, communications over standard networks, user functions, database
interfaces) a Soft PLC provides the most comprehensive industrial control
solution on the market. The GMT’s preferred choice for soft PLC is the TwinCAT
PLC automation software. TwinCAT PLC engine runs on a standard PC/IPC hardware.
It has a real-time kernel with a cycle time typically on the order of
microseconds, with a choice of fieldbus including EtherCAT. TwinCAT PLC offers
all the languages in the IEC 61131-3 standard and has a powerful development
environment for programs whose code size and data regions far exceed the
capacities of conventional PLC systems.</p>
</div></blockquote>
</div>
<div class="section" id="operating-systems">
<h3>6.1.3. <em>Operating Systems</em><a class="headerlink" href="#operating-systems" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><p>Low-latency, deterministic, system performance is required for real-time control
and signal processing applications that must respond to external events in a
predictable manner, independently of other considerations and particularly the
workload. Furthermore, low jitter operation of the fieldbus master is required
for some elaborate control loops (e.g., multi-axis interpolation) and for high
rate, low jitter, telemetry sampling. For those reasons, digital or motion
control applications are often built on top of a real-time operating system that
specially addresses:</p>
<blockquote>
<div><ul class="simple">
<li>concurrent, multi-tasking applications</li>
<li>task scheduling and priority policies</li>
<li>deterministic, low latency responses</li>
<li>interfaces to hardware</li>
</ul>
</div></blockquote>
<p>Several benchmarks <a class="reference internal" href="../bibliography/bibliography.html#bec12b" id="id8">[Bec12b]</a> and tests have been performed in order to assess the
adequacy of several Linux real-time extensions. The Linux kernel / GNU-Linux
operating system has a long track record of reliability for server and embedded
systems, and excellent soft real-time performance. RT_PREEMPT is very flexible
and actively developed, and has the performance needed for all control systems
requirements considered so far.</p>
<p>A typical control loop requires periodic execution at a predefined rate. Linux
offers different kinds of timers, depending on hardware availability. The High
Resolution Timers introduced in kernel 2.6 provide nanosecond resolution. The
next two Figures show the results of some of the tests performed. The tests
include also the performance of the EtherCAT master stack.</p>
<div class="figure" id="id18">
<img alt="../../_images/hi-res-timer-benchmark-2kHz.png" src="../../_images/hi-res-timer-benchmark-2kHz.png" />
<p class="caption"><span class="caption-number">Fig. 37 </span><span class="caption-text">High Resolution Timer Benchmark &#64; 2KHz</span></p>
</div>
<p>The above Figure shows a high resolution timer benchmark at 2KHz. In this
benchmark, a callback is scheduled to run every 500 microseconds, generating a
square waveform using a digital output channel on the host. The waveform is
captured on an oscilloscope where timing accuracy and jitter are measured.</p>
<div class="figure" id="id19">
<img alt="../../_images/hi-res-timer-benchmark-8kHz.png" src="../../_images/hi-res-timer-benchmark-8kHz.png" />
<p class="caption"><span class="caption-number">Fig. 38 </span><span class="caption-text">High Resolution Timer Benchmark with 3.5 Million Samples &#64; 8 kHz</span></p>
</div>
<p>The above Figure shows a high-resolution timer benchmark now at 8 kHz. A callback
is scheduled to run every 125 microsecond. The blue curve shows the waveform.
The oscilloscope triggers on the rising edge; the green curve can be ignored as
EtherCAT response was being tested in parallel. Over 3.5 million samples
collected, the signal shows a (static) clock skew of 330 nanoseconds (125.00 -
124.670), maximum jitter of 29 microseconds with 517 nanoseconds rms.</p>
</div></blockquote>
</div>
</div>
<div class="section" id="field-device-interface-technology">
<span id="field-device-interface"></span><h2>6.2. Field Device Interface Technology<a class="headerlink" href="#field-device-interface-technology" title="Permalink to this headline">¶</a></h2>
<p>Industrial fieldbuses have in the last few years moved from serial based
protocols like CAN or Profibus to Ethernet-based protocols. Ethernet-based
protocols are a successor of the VME backplanes used in previous generation
telescopes and have become one of the most common ways to connect input/output
modules with control processors, especially in industrial automation. These
protocols are based on industrial standards, which tend to be long-lived and
very stable, making them an ideal choice for the time span of the GMT project.
Among the different alternatives, EtherCAT is the most attractive: in addition
to being an ISO/IEC standard, several implementations of its components (e.g.,
master and slave stacks) are available on many platforms, including open source
alternatives.</p>
<p>EtherCAT is used to connect hardware devices (e.g., sensors and actuators) to
the fieldbus via Input/Output modules (IOModules). These modules implement
device profile standards based on the CAN Open specifications, for example the
CiA 402: CANopen device profile for drives and motion control (internationally
standardized in IEC 61800-7-201 and IEC 61800-7-301). In this way, the interface
protocol with local controllers is defined by standard profiles, often available
as XML files, thereby guaranteeing the availability of components from different
vendors. Interfaces based on industrial protocols reduce the systems engineering
burden and greatly facilitate the definition of control interfaces.</p>
<p>The field interface technology is greatly influenced by industrial automation
devices, which are becoming increasingly smart, as low-level motion controllers
are embedded directly in the field devices, reducing the need for separate
external control loops. With many control loops closed locally at the drives
level, the control computers in the electronic cabinets can be drastically
reduced in size and in many cases might not be needed at all.</p>
<p>The SWCS field device interface requirement is given by ELS-8030, Field Bus
Standard: The standard field bus shall be EtherCAT.</p>
<div class="section" id="ethercat">
<h3>6.2.1. <em>EtherCAT</em><a class="headerlink" href="#ethercat" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><p>The GMT standard Ethernet fieldbus is <a class="reference external" href="http://ethercat.org/en/ethercat.html">EtherCAT</a>, which is used to connect field
devices to DCCs (both PCs and PLCs) through IOModules. In general, all the
deployment cases can be grouped in:</p>
<blockquote>
<div><ul class="simple">
<li>DCCs connected to field devices via slave IOModules. This case is the most
common and is supported by a variety of different vendors. A very frequent
instance is, for example, Digital/Analog EtherCAT modules and motor drivers
– also called smart drives – which from a logical perspective are considered
IOModules, too.</li>
<li>DCCs connected to field devices directly. In this case the slave field
device has a fieldbus interface built in, for example, in the case of
EtherCAT encoders.</li>
</ul>
</div></blockquote>
<p>The Fieldbus cabling between DCCs and the first bus coupler is Ethernet. The
connection between the first bus coupler installed in the electronics room and
the rest of the bus couplers in the bus is optical fiber. This is done via an
optical fiber trunk cable to a distribution panel in a control cabinet inside
the telescope chamber. The control cabinets are located in strategic places of
the telescope structure. Inside the cabinet the field bus cable is converted
from fiber to copper. The cabling between the control cabinet and the device
power cabinets is done using CAT5e/CAT6 industrial Ethernet cables that are
robust, cost effective, and provide a reliable connection technology. The
IOModules are connected to DCCs using a cable redundancy arrangement. The
status of DC power supplies and their corresponding protections are monitored,
and potential free contacts are provided for the wiring in the GMT Power
cabinets.  The master fieldbus DCC uses the IgH EtherCAT master in order to
communicate with the IOModules connected to the fieldbus.</p>
<p>GMT has performed extensive testing connecting EtherCAT remote IOModules and
motion drives with real-time computers via fiber optics <a class="reference internal" href="../bibliography/bibliography.html#bec12c" id="id10">[Bec12c]</a>. The tests,
using cycle times between 0.1 and 0.5 milliseconds (time to read inputs,
process them and write outputs from the real time computer), have shown a
remarkable stability. This solution makes it possible to locate the control
computers in the electronics room instead of the telescope structure. The
consequences of this arrangement are: less heat dissipation, space and mass
required in the telescope enclosure, major cost reduction, and easier
maintenance. The Figure below shows an example of how GMT will deploy the DCC,
IOModules and field devices for the case of the Acquisition and Guiding
Wavefront Sensor System.</p>
<div class="figure" id="id20">
<img alt="../../_images/EtherCAT-AGWS.png" src="../../_images/EtherCAT-AGWS.png" />
<p class="caption"><span class="caption-number">Fig. 39 </span><span class="caption-text">EtherCAT topology sample for the Acquisition and Guiding Wavefront Sensor System.</span></p>
</div>
<p>GMT will adopt the CiA 402 CANopen device profile for drives and motion control,
which is suitable for frequency inverters, servo controllers, and stepper
motors. It is internationally standardized in IEC 61800-7-163 and IEC
61800-7-20164. The fact that the CiA402 profile type directly maps to EtherCAT
network technology in the IEC 61800-7-30165 standard speaks to the robustness
and long term stability of EtherCAT technology.</p>
</div></blockquote>
</div>
<div class="section" id="electronic-cabinets">
<h3>6.2.2. <em>Electronic Cabinets</em><a class="headerlink" href="#electronic-cabinets" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div>GMT has defined a Standard Electronics Cabinet (SEC) to be used by all
subsystems, to the extent possible, for the integration of their electronics.
The SEC includes a cooling system to maintain the air temperature inside the
cabinet near ambient and a power distribution unit for providing electrical
power to the subsystems. The SECs will be purchased, modified, and configured
by GMT and will be provided to the subsystem teams, as requested, for
integration and assembly of their electronics. Detailed specifications of the
SEC with drawings and envelopes, as well as wiring standards for GMT are
specified in the document GMT Electronics Standards <a class="reference internal" href="../bibliography/bibliography.html#sawy13a" id="id11">[Sawy13a]</a>.</div></blockquote>
</div>
<div class="section" id="terminal-blocks">
<h3>6.2.3. <em>Terminal Blocks</em><a class="headerlink" href="#terminal-blocks" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><p>Terminal Block technology plays an important role in the GMT design. Robust
wiring connections are essential to ensuring reliable and stable communication
between field elements and the electronics. Among the many variants existing
in the market today, the Push-In Technology (PIT) is preferred. PIT allows
connections whereby solid conductors or conductors with ferrules directly
insert into the terminal block. Only a flat-head screwdriver is necessary to
insert the wire and, in many cases, requires no tools at all. With PIT’s latch
function there is no need for special tools; using the integrated actuation
lever one can release the connected conductors quickly and easily. Vibration,
shock, and corrosion resistances are all covered by industrial standards
compliance. A few other Terminal Block features that are highly useful are
double function shafts that allow plug- in bridges and openings for test point
contacts. See the figure below for a couple of examples on the PIT terminal
block technology with bridges and test points adopted by GMT.</p>
<p>In the above Figure, Terminal Blocks in the SEC prototype at GMT. (Left)
Double function shafts have plug-in bridges and test contacts installed.
(Right) End clamp with auxiliary slot for storing bridges and double function
shafts on the main power terminal blocks.</p>
</div></blockquote>
<table border="1" class="docutils">
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="../../_images/terminal_blocks-left.png"><img alt="terminal_block1" class="align-middle" src="../../_images/terminal_blocks-left.png" style="width: 424.0px; height: 479.0px;" /></a></td>
<td><a class="reference internal" href="../../_images/terminal_blocks-right.png"><img alt="terminal_block2" class="align-top" src="../../_images/terminal_blocks-right.png" style="width: 424.0px; height: 479.0px;" /></a></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="integrated-prototypes">
<h3>6.2.4. <em>Integrated Prototypes</em><a class="headerlink" href="#integrated-prototypes" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><p>Several integrated tests have been performed at GMT with lab-assembled
prototypes. This implementation is set up with Linux servers running the
EtherCAT master, IO terminals from different vendors, custom made slaves,
drives commanding different types of motors (AC and steppers) and several
field sensors, like RTDs, PT100s, and load cells. The Figure below (left)
shows the first control panel prototype receiving the EtherCAT communications
from the DCCs over fiber with bus couplers, IOModules and motor drives in a
daisy-chain topology. The right-hand-side also shows the second-generation
control prototype, adding an Industrial PC with local control of the
electrical power startup sequence and load cell measurements, all with a
bus/tree topology. The Figure below shows the current prototype, which
includes all of the previous features in an electronics panel housed by a
Rittal TS8 cabinet. This is the design that drove the concept of GMT standard
electronics cabinet (SEC). Short cycle times in the order of 100us have been
achieved successfully with this setup.</p>
<div class="figure" id="id21">
<img alt="../../_images/control-panel-prototypes.png" src="../../_images/control-panel-prototypes.png" />
<p class="caption"><span class="caption-number">Fig. 40 </span><span class="caption-text">(Left) First and (Right) Second Generation Control Panel Prototypes</span></p>
</div>
<div class="figure" id="id22">
<img alt="../../_images/control-panel-proto-housed.png" src="../../_images/control-panel-proto-housed.png" />
<p class="caption"><span class="caption-number">Fig. 41 </span><span class="caption-text">Latest Control Panel Prototype Housed in a Rittal TS8 Enclosure.</span></p>
</div>
</div></blockquote>
</div>
</div>
<div class="section" id="time-distribution-system">
<span id="id12"></span><h2>6.3. Time Distribution System<a class="headerlink" href="#time-distribution-system" title="Permalink to this headline">¶</a></h2>
<p>While individual subsystems often provide their own internal clock(s), quality
and accuracy of (crystals) varies vastly between systems. A time distribution
system allows a heterogeneous set of controllers to reference and to synchronize
to the same master clock, and to share the same notional measure of time within
a distributed framework. Time systems are commonly layered into strata,
referring to how far each clock is located from the reference clock. For
telescope applications, a GPS clock (stratum 1) heads the hierarchy and acts as
the grand master clock.</p>
<p>A time system is particularly relevant for the GMT Telescope where time
synchronization plays an important role in many areas. In calculating a
telescope slew trajectory, for example, the telescope kernel pointing
coordinates must be computed using the GPS time; or for calculating continuous
drive demands, where it is necessary to interpolate between time stamps; or for
telemetry case, where uniform timestamps must be used to correlate real-time and
archive data.</p>
<div class="section" id="time-distribution-protocols">
<h3>6.3.1. <em>Time Distribution Protocols</em><a class="headerlink" href="#time-distribution-protocols" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><p>The most popular time distribution protocols are the Network Time Protocol
(NTP), the Inter-range Instrumentation Group code B (IRIG-B) and the Precision
Time Protocol (PTP). Their main characteristics are described in the following
list:</p>
<blockquote>
<div><ul class="simple">
<li>NTP is the de-facto standard for computer clusters. NTP assets include:
platform independence, availability on all operating systems, and simple
deployment relying on ubiquitous Ethernet connection. NTP precision is
limited by unaccounted network transmission delays, and susceptibility to
changes in the network topology.</li>
<li>IRIG-B is commonly used by 8-10m class telescopes and other major
experimental physics control systems. More accurate synchronization is
achieved in this case using a dedicated hardware clock and connection.
IRIG-B effectively builds a time bus concept that features specialized
components (hardware, software drivers, etc.) making it somewhat cumbersome
and often time-consuming to implement, adding extra burden to the deployment
(dedicated networks of coaxial cables are needed) and increasing the
implementation cost (specialized hardware for all sub-systems is required).</li>
<li>PTP emerged more recently in the distributed cluster world. It addresses
deficiencies of NTP, offering more accurate timestamps (thanks to
synchronization packets) and accounting for transmission delays in the
network. The second revision of the protocol achieves further accuracy by
using specialized network interface cards (NICs) and transparent clocks. The
NICs can provide hardware timestamp, so the nearer to the hardware the
timestamps are generated the higher the precision.</li>
</ul>
</div></blockquote>
<p>The main features of the relevant time protocols are summarized in the Table
below:</p>
<table border="1" class="docutils" id="id23">
<caption><span class="caption-number">Table 25 </span><span class="caption-text">Time Protocols Comparison</span><a class="headerlink" href="#id23" title="Permalink to this table">¶</a></caption>
<colgroup>
<col width="10%" />
<col width="19%" />
<col width="14%" />
<col width="56%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Protocol</th>
<th class="head">Sync. Accuracy</th>
<th class="head">Interconnect</th>
<th class="head">Properties</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>NTP</td>
<td>~ 2 ms</td>
<td>LAN/WAN</td>
<td>Subject to network delays, affected by topology changes</td>
</tr>
<tr class="row-odd"><td>Protocol</td>
<td>Sync. Accuracy</td>
<td>Interconnect</td>
<td>Properties</td>
</tr>
<tr class="row-even"><td>IRIG-B</td>
<td>1-10 us</td>
<td>Coaxial</td>
<td>Needs dedicated hardware and software drivers</td>
</tr>
<tr class="row-odd"><td rowspan="2">PTP</td>
<td>5-50 us (software)</td>
<td>LAN</td>
<td>Measure and compensate transmission delays</td>
</tr>
<tr class="row-even"><td>&lt; 1 us (hardware)</td>
<td>&nbsp;</td>
<td>Better accuracy using dedicated NICs and switches</td>
</tr>
</tbody>
</table>
</div></blockquote>
</div>
<div class="section" id="precision-time-protocol">
<h3>6.3.2. <em>Precision Time Protocol</em><a class="headerlink" href="#precision-time-protocol" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div>A growing number of industrial and power utility tasks require the use of
time-synchronized networks. Traditionally, timing information was distributed
via a dedicated network separate from the main application, such as real-time
distribution protocols, of which IRIG-B is perhaps the best known. More
recently, the market has shifted to a single converged data and distributed
timing network based on standard Ethernet running IEEE1588 Precision Time
Protocol as part of the application layer. The protocol can achieve
sub-microsecond synchronization performance over distributed networks. The
IEEE15888 Standard is currently in its 2nd revision.</div></blockquote>
</div>
<div class="section" id="ptp-software-stacks-for-linux">
<h3>6.3.3. <em>PTP Software Stacks for Linux</em><a class="headerlink" href="#ptp-software-stacks-for-linux" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div>Several open source (e.g., ptpd-2, ptppd-phc) and commercial (e.g., IXXAT,
FSMLabs) software stacks are available for Linux, including network drivers
with hardware timestamp (e.g., Blackfin on-chip Ethernet MAC driver, Intel
Gigabit Ethernet Network Driver). Refer to the report IEEE- 1588 PTP
Evaluation <a class="reference internal" href="../bibliography/bibliography.html#bec12d" id="id13">[Bec12d]</a> for a detailed list and analysis on the software stacks for
Linux.</div></blockquote>
</div>
<div class="section" id="lab-evaluation">
<h3>6.3.4. <em>Lab Evaluation</em><a class="headerlink" href="#lab-evaluation" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><p>Several PTP topologies to measure synchronization accuracy have been tested at
the GMT lab67. A common arrangement is shown in Figure below:</p>
<div class="figure" id="id24">
<img alt="../../_images/PTP-master-slaves.png" src="../../_images/PTP-master-slaves.png" />
<p class="caption"><span class="caption-number">Fig. 42 </span><span class="caption-text">PTP Master with 2 Slaves Arrangement using a Switched Network and a Dedicated
NIC.</span></p>
</div>
<p>Two slave computers were synchronized with PTP and each was generating a
digital pulse over the serial communications port at 10 Hz. The test was
carried out over a private LAN using one dedicated NIC isolated from the
intranet and another NIC that was running the EtherCAT master in the
background. The daemon used for the test was the ptpd-2 open source stack.</p>
<p>Synchronization accuracy was measured with an oscilloscope, capturing the
delay and variation between the two independently generated pulses as shown in
the Figure below. Over one million samples – which at 10 pulses per second
translates roughly into 27 hours – the standard deviation is less than 2.5us
and representative of PTP stability. The min/max outliers amplitude at &lt; 25us
falls within the technology accepted published figures.</p>
<div class="figure" id="id25">
<img alt="../../_images/PTP-synchronization-accuracy.png" src="../../_images/PTP-synchronization-accuracy.png" />
<p class="caption"><span class="caption-number">Fig. 43 </span><span class="caption-text">Lab PTP Synchronization Accuracy Tests</span></p>
</div>
<p>During the tests a couple of disturbances were introduced to evaluate the
synchronization robustness. First some network traffic was initiated,
essentially copying the entire file system from the master machine to an
external computer outside the PTP LAN (directing the traffic to the general
LAN). The accuracy in this case did not seem to be affected. Next, the
machine was artificially loaded at lower priority, leaving the non-isolated
CPU with 100% effective load. No evident effect on timing accuracy was
observed.</p>
<p>A summary with all the measured quantities using a ptpd-2 implementation of
the Linux PTP Software Stack is presented in the following Table:</p>
<table border="1" class="docutils" id="id26">
<caption><span class="caption-number">Table 26 </span><span class="caption-text">ptpd-2 Implementation Measured Accuracy</span><a class="headerlink" href="#id26" title="Permalink to this table">¶</a></caption>
<colgroup>
<col width="59%" />
<col width="41%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Statistic</th>
<th class="head">Value</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>Samples (pulses)
Mean offset
Standard deviation
Min/Max amplitude</td>
<td>&gt; 1 million
1.70 us
2.48 us
24 us</td>
</tr>
</tbody>
</table>
<p>The PTP network tested before was reconfigured to work over the common LAN,
using now the same NIC for PTP synchronization and general networking. The
table below shows the statistics obtained over the common LAN case, and as
expected, the performance is slightly worse (about 60%) than using a dedicated
LAN.</p>
<table border="1" class="docutils" id="id27">
<caption><span class="caption-number">Table 27 </span><span class="caption-text">ptpd-2 Measured Accuracy over a Common LAN</span><a class="headerlink" href="#id27" title="Permalink to this table">¶</a></caption>
<colgroup>
<col width="59%" />
<col width="41%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Statistic</th>
<th class="head">Value</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>Samples (pulses)
Mean offset
Standard deviation
Min/Max amplitude</td>
<td>278 k
5.60 us
4.21 us
35.4 us</td>
</tr>
</tbody>
</table>
<p>For optimal performance with EtherCAT, a patched driver for the fieldbus
enabled NIC was used. During the tests, some interactions between the PTP
daemon and the driver were observed, right when the EtherCAT master was
started and the patched driver for the NIC got loaded. The issue can be
addressed by sequencing the access to the NIC, for example by stopping the PTP
daemon while EtherCAT initializes and then running it again after EtherCAT has
started.</p>
<p>In summary, PTP has several advantages for time distribution and
synchronization in a widely distributed system as GMT:</p>
<blockquote>
<div><ul class="simple">
<li>Easy and inexpensive to implement using a LAN infrastructure</li>
<li>Common and proven technology</li>
<li>Guaranteed long time support</li>
</ul>
</div></blockquote>
<p>However, a pure software solution may not be enough for a system requiring
very high timing resolution stability. The numbers collected during these
tests are consistent with the literature and leave no reason to doubt hardware
solutions could bring sub-microsecond accuracy. For the GMT Telescope Control,
a more detailed system analysis should be carried out if 5-50 microsecond
accuracy is necessary.</p>
</div></blockquote>
</div>
</div>
<div class="section" id="distributed-communications-middleware">
<span id="distributed-communications"></span><h2>6.4. Distributed Communications - Middleware<a class="headerlink" href="#distributed-communications-middleware" title="Permalink to this headline">¶</a></h2>
<p>In a distributed control system the use of middleware enables communication
between different components deployed in different processes or computers. This
makes middleware software pervasive. A middleware agnostic architecture allows
the decoupling of applications from the specific implementation. The SWCS
communication requirement is given by SWC-6879, Communication Protocols: SWCS
shall define a set of software protocols and APIs that allow the communication
with the required performance between different components.</p>
<p>ZeroMQ is used for communications between distributed components, chosen for its
desirable features. Given that the project is in early stages and that the
infrastructure software is likely to continue evolving, this selection can be
revised in the future as fitting. Front–end software deployed in a browser uses
WebSockets to communicate with the rest of the distributed components. The
serialization/deserialization of transmitted data packages is accomplished using
<a class="reference external" href="http://msgpack.org/">MessagePack</a>.</p>
<p>GMT is developing several prototypes to validate the use of these technologies.
These prototypes include the use of <a class="reference external" href="http://en.wikipedia.org/wiki/NoSQL">NoSQL</a> databases like Mongodb, scripting
languages like <a class="reference external" href="http://en.wikipedia.org/wiki/CoffeeScript">Coffeescript</a>, user
interface libraries based on HTML5 and SVG like <a class="reference external" href="http://d3js.org">d3.js</a>,
event-driven environments like node.js or high performance middleware libraries
like zeromq, MessagePack or ProtocolBuffers.</p>
</div>
<div class="section" id="networking">
<span id="id14"></span><h2>6.5. Networking<a class="headerlink" href="#networking" title="Permalink to this headline">¶</a></h2>
<p>Telescope Control Systems span a large dynamic range in bandwidth and latency
requirements. Most systems have fairly relaxed requirements on bandwidth and
latency, e.g., the telescope main axis require few parameters to update at
nominally 20 Hz, which is easily handled by off-the-shelf Ethernet components. A
few other systems such as wavefront control and Adaptive Optics run at kHz
regime, shuffling orders of magnitude larger data volume in real time to read
out telemetry and WFS, computing wavefront slopes, commanding ASMs, etc.</p>
<p>The global trend in High Performance Computing and Ethernet communications
drives rapid developments in the industry, somewhat comparable to CPUs (Moore&#8217;s
law) seen in the previous decade. The emphasis on inter-connected cloud
computing, and the ever-growing bandwidth and low latency requirements, continue
to drive the technology today. Recent trends from high performance computing
community, coupled with cost-benefit components, point to Infiniband* as a safe
choice in the present and for future growth.</p>
<div class="section" id="control-network">
<h3>6.5.1. <em>Control Network</em><a class="headerlink" href="#control-network" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><p>The GMT control network design is based on an Ethernet switched-fabric topology
that eliminates the existence of single point failures. The control network is
the backbone that communicates with any subsystem of the Telescope. It connects
the control room, the electronics room (where most of the computers are
located), and all field interface modules installed on the telescope structure.
The network is deployed using optical fiber trunk cables between the telescope
enclosure base and the facility building. The <a class="reference internal" href="../architecture/overall_architecture.html#figure-supervisory-hierarchy"><span class="std std-ref">Supervisory Hierarchy Figure</span></a> and Figure 10-26 (TBC) illustrate the conceptual
design of the control network.</p>
<p>Networking equipment evolves rapidly, thus it is advisable to wait as close as
possible to the deployment phase for final procurement. In this early stage of
the project a Gigabit Ethernet network is used to prototype the communication
system. This prototype will evolve during the life of the project to track
technology evolution and to optimize the final network design.</p>
</div></blockquote>
</div>
<div class="section" id="ultra-low-latency-network">
<h3>6.5.2. <em>Ultra-Low Latency Network</em><a class="headerlink" href="#ultra-low-latency-network" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><p>A low latency, high bandwidth, and scalable, communication backbone is a
prerequisite for the design of a modular adaptive and active wavefront control
system. For this purpose, a 40 Gbps Infiniband network has been tested in the
lab. The tests include a combination of high bandwidth middleware, Infiniband
uverbs, MPI† (Message Passing Interface) and RDMA‡ (Remote Direct Memory Access)
on a mini-cluster setup. Preliminary results show that existing ultra-low
latency, off-the-shelf, communication products provide the required performance
out of the box.</p>
<p>Infiniband support was originally provided by the OpenFabrics Enterprise
Distribution. Now the support has been merged into the upstream Linux kernel.
User-space support packages (e.g., subnet manager and performance tools) are
made available through regular distribution channels. Infiniband implements the
following key features:</p>
<blockquote>
<div><ul class="simple">
<li>Kernel bypass – Removes the need for context switches between kernel and
user-space.</li>
<li>RDMA enabled – Application can post commands directly to the host adapter
eliminating expensive calls and associated (latency) overheads of going
through an OS stack.</li>
<li>Zero copy – Eliminates buffering and allows direct access on remote servers,
significantly reducing latency.</li>
</ul>
</div></blockquote>
</div></blockquote>
</div>
<div class="section" id="benchmark-tests">
<h3>6.5.3. <em>Benchmark Tests</em><a class="headerlink" href="#benchmark-tests" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><p>A test environment was deployed consisting three hosts interconnected with an
8-port 40 Gb/s switch, using the following components:</p>
<blockquote>
<div><ul class="simple">
<li>Mellanox ConnectX-2 Single 4x QSFP 40Gb/s ($562.00/unit)</li>
<li>Mellanox Infiniscale IV QDR 8-port switch ($1,750.00)</li>
</ul>
</div></blockquote>
<p>The set-up is shown in the Figure below. On the left the entire computer cabinet
can be seen including the Mellanox switch on top and the Linux machines at the
bottom. A close-up on the Mellanox switch is depicted on the right.</p>
<blockquote>
<div></div></blockquote>
<table border="1" class="docutils">
<colgroup>
<col width="42%" />
<col width="58%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="../../_images/ultra_low_latency_networking1.png"><img alt="ULL_figure1" class="align-middle" src="../../_images/ultra_low_latency_networking1.png" style="width: 381.0px; height: 401.0px;" /></a></td>
<td><a class="reference internal" href="../../_images/ultra_low_latency_networking2.png"><img alt="ULL_figure2" class="align-top" src="../../_images/ultra_low_latency_networking2.png" style="width: 538.0px; height: 401.0px;" /></a></td>
</tr>
</tbody>
</table>
<p>Measured latency (typical and worst-case) as a function of the payload size in
bytes is shown in the Figure below. This benchmark used Reliable Connection
single queued pairs. As advertised, the typical latency for data write, send,
and read is 1-2 μs for payloads &lt;256 bytes, increasing to ~8 μs for payloads of
~16 kB, such as an ASM command vector.</p>
<div class="figure" id="id28">
<img alt="../../_images/latency_vs_payload.png" src="../../_images/latency_vs_payload.png" />
<p class="caption"><span class="caption-number">Fig. 44 </span><span class="caption-text">Measured Latency vs. Payload</span></p>
</div>
<p>Another benchmark test was run, this time for latency jitter using again
Reliable Connection single queued pairs.  As shown in the Figure below, for
small message sizes (2 bytes), &lt;0.1% of messages have a latency exceeding 3 μs.
Both tests were run under a mainstream Linux kernel, RT_PREEMPT and RTAI, giving
similar numbers independently of the platform.</p>
<div class="figure" id="id29">
<img alt="../../_images/latency_distribution.png" src="../../_images/latency_distribution.png" />
<p class="caption"><span class="caption-number">Fig. 45 </span><span class="caption-text">Latency Distribution</span></p>
</div>
<p>The evaluation of Infiniband shows that the technology can be easily deployed at
relatively low cost.  The performance measured agrees with the vendor advertised
capabilities without the need for specific or complex tuning.  While the verb
API is very specific to RDMA, it is well documented, and most of the complexity
stems from connection management, which can be leveraged using helper libraries
(<em>rdma_cm</em> or other out-of-band mechanism).</p>
<p>The technology is already widely deployed and mainstream in the high performance
computing community.  Driven by community needs, process improvement is under
active development and there is every expectation that new products will improve
substantially in performance in near future.</p>
</div></blockquote>
</div>
</div>
<div class="section" id="software-development-platform">
<span id="id15"></span><h2>6.6. Software Development Platform<a class="headerlink" href="#software-development-platform" title="Permalink to this headline">¶</a></h2>
<p>The Toolchain or Software Development Kit (SDK) includes all the required
resources to develop the GMT software.  These resources include:</p>
<blockquote>
<div><ul class="simple">
<li>Workstation operating system.  The workstation operating system shall be Linux.</li>
<li>Programming languages.  The programming languages that can be used in the
GMT software development are:<ul>
<li>ANSI C++ for general application programming in the LCU platform and for
performance sensitive application in the workstation deployment platform.</li>
<li>ANSI C for driver programming in the LCU programming platform.</li>
<li>Python for data pipelines.</li>
<li>Coffescript/Javascript for graphical programing and general applications
in the workstation deployment platform.</li>
<li>IEC 61131-3 for PLC embedded applications.</li>
</ul>
</li>
<li>C/C++ Compilers.  The C/C++ compilers to be used in the workstation
deployment platform shall be: GNU g++ version 4.2 or above.  The C/C++
compilers to be used in the real-time deployment platform shall be: GNU g++
version 4.2 or above.</li>
<li>Javascript Virtual Machine.  The Javascript virtual machine to be used in
the workstation deployment platform shall be Google V8.  The V8 virtual
machine is available as part of the node.js system</li>
<li>Coffescript compiler.  Javascript code shall be generated using the
coffescript compiler.</li>
<li>Distributed Programming environment.  Distributed applications that are not
performance sensitive shall be developed using the node.js system.</li>
<li>Python interpreter.  Python programs shall be compatible with Python version
TBD.</li>
<li>The Figure below provides a summary of the GMT software development platform.</li>
</ul>
</div></blockquote>
<div class="figure" id="id30">
<img alt="../../_images/software_platform.png" src="../../_images/software_platform.png" />
<p class="caption"><span class="caption-number">Fig. 46 </span><span class="caption-text">GMT Software Platform Summary</span></p>
</div>
</div>
</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../process/process_introduction.html" class="btn btn-neutral float-right" title="7. Process" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../frameworks/component_frameworks.html" class="btn btn-neutral" title="5. Frameworks" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2015, contributors.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../',
            VERSION:'1.0-rc1',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../_static/doctools.js"></script>

  

  
  
    <script type="text/javascript" src="../../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>